{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1LE9xedmmnK5CpSFMVAfm3-SWgdGHm4Xb","timestamp":1665397507456}],"mount_file_id":"1pMwmBVjf5EPD9P-VAdad5NAgiLbhXkke","authorship_tag":"ABX9TyNvXaM8qC+O6iNQ9uD5tVOx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["### Mounting drive and installing packages"],"metadata":{"id":"9NcmJ_wzwGPe"}},{"cell_type":"code","source":["# mounting drive to import data and stopword list\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"XxSKrsYXDcvX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# installing packages\n","!pip install bertopic\n","!pip install joblib==1.1.0"],"metadata":{"id":"KRd67myudi3t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Playing with Topic Modelling"],"metadata":{"id":"nCAAOLsUBsTZ"}},{"cell_type":"markdown","source":["Loading the data and stopword list"],"metadata":{"id":"-XWxbsxFkHRc"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","\n","# load data\n","df_original = pd.read_csv(\"file_name.csv\")\n","\n","# keeps the original data intact for when saving the output\n","df_all = df_original.copy()\n","\n","# load stopwords. This should result in a list. The following work for a txt-file\n","with open('stopword_list_name.txt') as f:\n","    stopwords = f.readlines()\n","\n","# saving all stopwords in a list\n","for i, val in enumerate(stopwords):\n","  new_line = val.replace(\"\\n\", \"\")\n","  stopwords[i] = new_line"],"metadata":{"id":"T7ucsN2SwSkR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pre-processing Example"],"metadata":{"id":"Rll-14fvt28M"}},{"cell_type":"code","source":["# suggestion for pre-processing of Twitter data. This can be skipped if no additional pre-processing is needed\n","# in the following we pretend a column in the pandas dataframe df_all is called \"text\". This can easily be changed to the appropriate column name\n","df_all.text = df_all.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1) # example of removing links\n","df_all.text = df_all.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1) # eaxmple of removing twitter handles\n","df_all.text = df_all.apply(lambda row: \" \".join(re.sub(\"[^a-zA-Z]+\", \" \", row.text).split()), 1) # example of removing symbols"],"metadata":{"id":"eY3CP8tknqWF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# stopwords should not be removed, because BERTopic need them for contextual information. \n","# However, if the data contains words that supply no contextual information these can be removed with the following \n","\n","def remove_random_words(sentence, stopword_list):\n","    tokens = sentence.split(\" \")\n","    tokens_filtered= [word for word in tokens if not word in stopword_list]\n","    return (\" \").join(tokens_filtered)\n","\n","# list of words with no contextual information\n","random_words = [\"\"]\n","\n","df_all.text = [remove_random_words(sentence, random_words) for sentence in df_all.text] # removing words in the list \"random_words\""],"metadata":{"id":"I27Jwq3tnGQ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Saving all documents as list"],"metadata":{"id":"_-etqLeYt8Ct"}},{"cell_type":"code","source":["documents = df_all.text.to_list() # saving the pre-processed text as a list for BERTopic"],"metadata":{"id":"qbICScXtnUL1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creating topics with BERTopic"],"metadata":{"id":"z1PsxHDakMFB"}},{"cell_type":"code","source":["from hdbscan import HDBSCAN\n","import numpy as np\n","from umap import UMAP\n","from sklearn.feature_extraction.text import CountVectorizer\n","from bertopic import BERTopic\n","\n","#umap_model = UMAP(random_state=550) # if applicable, the random state of UMAP can be fixed to make the topics reproducible\n","\n","# used for disregarding the stopwords in the stopword list, when generating topic representations\n","# this is useful for generating more easily interpretable topics\n","vectorizer_model = CountVectorizer(stop_words = stopwords)\n","\n","# initialising an instance of HDBSCAN with appropriate parameters\n","hdbscan_model = HDBSCAN(min_cluster_size=70, # minimum number of documents in each cluster\n","                        metric='euclidean', # metric used for calculating distance in vector space\n","                        prediction_data=True, # this speeds up later predictions\n","                        min_samples=10 # indicating how conservative the clusterings algorithm should be when it comes to outliers. Higher values generete larger outlier topic\n","                        )\n","\n","# creating the wanted BERTopic instance\n","topic_model = BERTopic(embedding_model=\"paraphrase-multilingual-mpnet-base-v2\", # specifying to use the multilingual model. Alternative is \"all-mpnet-base-v2\" for English\n","                           nr_topics = \"auto\", # \"auto\" merge similar topics. Alternatively a number of topics can be specified\n","                           calculate_probabilities=True, # calculates the probabilities of all topics for each document\n","                           vectorizer_model=vectorizer_model, # using the CountVectorizer from above to remove stopwords from topic representations\n","                           hdbscan_model = hdbscan_model # using the HDBSCAN settings initialised above\n","                           #umap_model=umap_model # only use if UMAP model should be used for replication of results\n","                           #diversity=0.2 # Additional parameter that can be used to diversify the resulting topic representations. If set to None, MMR will not be used.\n","                           )\n","\n","# applying BERTopic to the data\n","topics, probabilities = topic_model.fit_transform(documents)\n"],"metadata":{"id":"y07HHCh2259i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inspecting the topics with top 4 words from topic representations"],"metadata":{"id":"27hYjpByt_v1"}},{"cell_type":"code","source":["topic_model.get_topic_info()"],"metadata":{"id":"4NxS9tUhry6G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Assigning additional documents to each topic group (only if applicable to the specific use case)"],"metadata":{"id":"4ksKt3LekYIR"}},{"cell_type":"code","source":["# threshold to use for assigning additional tweets to each topic group\n","probability_threshold = 0.30\n","\n","# assigning additional documents to each topic if probability above 0.30\n","topics = [np.argmax(prob) if max(prob) >= probability_threshold else topics[idx] for idx, prob in enumerate(probabilities)]"],"metadata":{"id":"ywAe7fk9N8eP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Saving the data now with column indicating topics"],"metadata":{"id":"G85kGWuoOICd"}},{"cell_type":"code","source":["# adding topics to original data\n","df_original[\"topic\"] = topics\n","\n","# saving the output of BERTopic analysis. As minimum this will contain a column with the original text from each document and the topics\n","df_original.to_csv(\"file_name.csv\")"],"metadata":{"id":"vVGE4BnI3n-z"},"execution_count":null,"outputs":[]}]}